{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeplearning.ai - TensorFlow in Practice - Course 3 - Week 4 - Lesson 1 - Simple Word Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikful/dlai-tf-in-practice/blob/master/TensorFlow_in_Practice_Course_3_Week_4_Lesson_1_Simple_Word_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0_N_1AXBdL4",
        "colab_type": "text"
      },
      "source": [
        "# Next word output generation using traditional Irish Song as training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOwsuGQQY9OL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c394ec0e-457f-45bb-e8f3-9f36b1a6506b"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(f'TensorFlow version: {tf.__version__}')\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PRnDnCW-Z7qv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9549c974-0efb-4d31-cb1c-0172f26c2931"
      },
      "source": [
        "tokenizer = Tokenizer() # instantiate tokenizer\n",
        "\n",
        "data=\"In the town of Athy one Jeremy Lanigan \\n Battered away til he hadnt a pound. \\nHis father died and made him a man again \\n Left him a farm and ten acres of ground. \\nHe gave a grand party for friends and relations \\nWho didnt forget him when come to the wall, \\nAnd if youll but listen Ill make your eyes glisten \\nOf the rows and the ructions of Lanigans Ball. \\nMyself to be sure got free invitation, \\nFor all the nice girls and boys I might ask, \\nAnd just in a minute both friends and relations \\nWere dancing round merry as bees round a cask. \\nJudy ODaly, that nice little milliner, \\nShe tipped me a wink for to give her a call, \\nAnd I soon arrived with Peggy McGilligan \\nJust in time for Lanigans Ball. \\nThere were lashings of punch and wine for the ladies, \\nPotatoes and cakes; there was bacon and tea, \\nThere were the Nolans, Dolans, OGradys \\nCourting the girls and dancing away. \\nSongs they went round as plenty as water, \\nThe harp that once sounded in Taras old hall,\\nSweet Nelly Gray and The Rat Catchers Daughter,\\nAll singing together at Lanigans Ball. \\nThey were doing all kinds of nonsensical polkas \\nAll round the room in a whirligig. \\nJulia and I, we banished their nonsense \\nAnd tipped them the twist of a reel and a jig. \\nAch mavrone, how the girls got all mad at me \\nDanced til youd think the ceiling would fall. \\nFor I spent three weeks at Brooks Academy \\nLearning new steps for Lanigans Ball. \\nThree long weeks I spent up in Dublin, \\nThree long weeks to learn nothing at all,\\n Three long weeks I spent up in Dublin, \\nLearning new steps for Lanigans Ball. \\nShe stepped out and I stepped in again, \\nI stepped out and she stepped in again, \\nShe stepped out and I stepped in again, \\nLearning new steps for Lanigans Ball. \\nBoys were all merry and the girls they were hearty \\nAnd danced all around in couples and groups, \\nTil an accident happened, young Terrance McCarthy \\nPut his right leg through miss Finnertys hoops. \\nPoor creature fainted and cried Meelia murther, \\nCalled for her brothers and gathered them all. \\nCarmody swore that hed go no further \\nTil he had satisfaction at Lanigans Ball. \\nIn the midst of the row miss Kerrigan fainted, \\nHer cheeks at the same time as red as a rose. \\nSome of the lads declared she was painted, \\nShe took a small drop too much, I suppose. \\nHer sweetheart, Ned Morgan, so powerful and able, \\nWhen he saw his fair colleen stretched out by the wall, \\nTore the left leg from under the table \\nAnd smashed all the Chaneys at Lanigans Ball. \\nBoys, oh boys, twas then there were runctions. \\nMyself got a lick from big Phelim McHugh. \\nI soon replied to his introduction \\nAnd kicked up a terrible hullabaloo. \\nOld Casey, the piper, was near being strangled. \\nThey squeezed up his pipes, bellows, chanters and all. \\nThe girls, in their ribbons, they got all entangled \\nAnd that put an end to Lanigans Ball.\"\n",
        "\n",
        "corpus = data.lower().split(\"\\n\") # create corpus from data\n",
        "\n",
        "tokenizer.fit_on_texts(corpus) # fit tokenizer to corpus\n",
        "total_words = len(tokenizer.word_index) + 1 \n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'and': 1, 'the': 2, 'a': 3, 'in': 4, 'all': 5, 'i': 6, 'for': 7, 'of': 8, 'lanigans': 9, 'ball': 10, 'were': 11, 'at': 12, 'to': 13, 'she': 14, 'stepped': 15, 'his': 16, 'girls': 17, 'as': 18, 'they': 19, 'til': 20, 'he': 21, 'again': 22, 'got': 23, 'boys': 24, 'round': 25, 'that': 26, 'her': 27, 'there': 28, 'three': 29, 'weeks': 30, 'up': 31, 'out': 32, 'him': 33, 'was': 34, 'spent': 35, 'learning': 36, 'new': 37, 'steps': 38, 'long': 39, 'away': 40, 'left': 41, 'friends': 42, 'relations': 43, 'when': 44, 'wall': 45, 'myself': 46, 'nice': 47, 'just': 48, 'dancing': 49, 'merry': 50, 'tipped': 51, 'me': 52, 'soon': 53, 'time': 54, 'old': 55, 'their': 56, 'them': 57, 'danced': 58, 'dublin': 59, 'an': 60, 'put': 61, 'leg': 62, 'miss': 63, 'fainted': 64, 'from': 65, 'town': 66, 'athy': 67, 'one': 68, 'jeremy': 69, 'lanigan': 70, 'battered': 71, 'hadnt': 72, 'pound': 73, 'father': 74, 'died': 75, 'made': 76, 'man': 77, 'farm': 78, 'ten': 79, 'acres': 80, 'ground': 81, 'gave': 82, 'grand': 83, 'party': 84, 'who': 85, 'didnt': 86, 'forget': 87, 'come': 88, 'if': 89, 'youll': 90, 'but': 91, 'listen': 92, 'ill': 93, 'make': 94, 'your': 95, 'eyes': 96, 'glisten': 97, 'rows': 98, 'ructions': 99, 'be': 100, 'sure': 101, 'free': 102, 'invitation': 103, 'might': 104, 'ask': 105, 'minute': 106, 'both': 107, 'bees': 108, 'cask': 109, 'judy': 110, 'odaly': 111, 'little': 112, 'milliner': 113, 'wink': 114, 'give': 115, 'call': 116, 'arrived': 117, 'with': 118, 'peggy': 119, 'mcgilligan': 120, 'lashings': 121, 'punch': 122, 'wine': 123, 'ladies': 124, 'potatoes': 125, 'cakes': 126, 'bacon': 127, 'tea': 128, 'nolans': 129, 'dolans': 130, 'ogradys': 131, 'courting': 132, 'songs': 133, 'went': 134, 'plenty': 135, 'water': 136, 'harp': 137, 'once': 138, 'sounded': 139, 'taras': 140, 'hall': 141, 'sweet': 142, 'nelly': 143, 'gray': 144, 'rat': 145, 'catchers': 146, 'daughter': 147, 'singing': 148, 'together': 149, 'doing': 150, 'kinds': 151, 'nonsensical': 152, 'polkas': 153, 'room': 154, 'whirligig': 155, 'julia': 156, 'we': 157, 'banished': 158, 'nonsense': 159, 'twist': 160, 'reel': 161, 'jig': 162, 'ach': 163, 'mavrone': 164, 'how': 165, 'mad': 166, 'youd': 167, 'think': 168, 'ceiling': 169, 'would': 170, 'fall': 171, 'brooks': 172, 'academy': 173, 'learn': 174, 'nothing': 175, 'hearty': 176, 'around': 177, 'couples': 178, 'groups': 179, 'accident': 180, 'happened': 181, 'young': 182, 'terrance': 183, 'mccarthy': 184, 'right': 185, 'through': 186, 'finnertys': 187, 'hoops': 188, 'poor': 189, 'creature': 190, 'cried': 191, 'meelia': 192, 'murther': 193, 'called': 194, 'brothers': 195, 'gathered': 196, 'carmody': 197, 'swore': 198, 'hed': 199, 'go': 200, 'no': 201, 'further': 202, 'had': 203, 'satisfaction': 204, 'midst': 205, 'row': 206, 'kerrigan': 207, 'cheeks': 208, 'same': 209, 'red': 210, 'rose': 211, 'some': 212, 'lads': 213, 'declared': 214, 'painted': 215, 'took': 216, 'small': 217, 'drop': 218, 'too': 219, 'much': 220, 'suppose': 221, 'sweetheart': 222, 'ned': 223, 'morgan': 224, 'so': 225, 'powerful': 226, 'able': 227, 'saw': 228, 'fair': 229, 'colleen': 230, 'stretched': 231, 'by': 232, 'tore': 233, 'under': 234, 'table': 235, 'smashed': 236, 'chaneys': 237, 'oh': 238, 'twas': 239, 'then': 240, 'runctions': 241, 'lick': 242, 'big': 243, 'phelim': 244, 'mchugh': 245, 'replied': 246, 'introduction': 247, 'kicked': 248, 'terrible': 249, 'hullabaloo': 250, 'casey': 251, 'piper': 252, 'near': 253, 'being': 254, 'strangled': 255, 'squeezed': 256, 'pipes': 257, 'bellows': 258, 'chanters': 259, 'ribbons': 260, 'entangled': 261, 'end': 262}\n",
            "263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soPGVheskaQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create input sequences - training data\n",
        "\n",
        "# Take each sequence (x) and the next word (y) combination from the data\n",
        "# (i.e. x = word1, y = word2. Then: x = word1, word2, y= word3 etc. etc.)\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "# one-hot encode the y/label data\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJtwVB2NbOAP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d312cdc4-3d44-4abb-dd08-f3bedb0cc6a8"
      },
      "source": [
        "print(tokenizer.word_index['in'])\n",
        "print(tokenizer.word_index['the'])\n",
        "print(tokenizer.word_index['town'])\n",
        "print(tokenizer.word_index['of'])\n",
        "print(tokenizer.word_index['athy'])\n",
        "print(tokenizer.word_index['one'])\n",
        "print(tokenizer.word_index['jeremy'])\n",
        "print(tokenizer.word_index['lanigan'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "2\n",
            "66\n",
            "8\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49Cv68JOakwv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8379447e-5906-4e93-cc58-a9721770125a"
      },
      "source": [
        "print(xs[6]) # print the 6th sequence x data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  4  2 66  8 67 68 69]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY-jwvfgbEF8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "02258c15-1d2e-4a8e-8d21-d97c9610b768"
      },
      "source": [
        "print(ys[6])# print the 6th sequence y one-hot encoded data"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtzlUMYadhKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "90861115-9c4f-4761-84b3-ae6009d8bfda"
      },
      "source": [
        "print(xs[5]) # print the 5th sequence x data\n",
        "print(ys[5]) # print the 5th sequence y data"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  0  4  2 66  8 67 68]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4myRpB1c4Gg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a9ade4b4-346b-4211-f115-7afde554c243"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'and': 1, 'the': 2, 'a': 3, 'in': 4, 'all': 5, 'i': 6, 'for': 7, 'of': 8, 'lanigans': 9, 'ball': 10, 'were': 11, 'at': 12, 'to': 13, 'she': 14, 'stepped': 15, 'his': 16, 'girls': 17, 'as': 18, 'they': 19, 'til': 20, 'he': 21, 'again': 22, 'got': 23, 'boys': 24, 'round': 25, 'that': 26, 'her': 27, 'there': 28, 'three': 29, 'weeks': 30, 'up': 31, 'out': 32, 'him': 33, 'was': 34, 'spent': 35, 'learning': 36, 'new': 37, 'steps': 38, 'long': 39, 'away': 40, 'left': 41, 'friends': 42, 'relations': 43, 'when': 44, 'wall': 45, 'myself': 46, 'nice': 47, 'just': 48, 'dancing': 49, 'merry': 50, 'tipped': 51, 'me': 52, 'soon': 53, 'time': 54, 'old': 55, 'their': 56, 'them': 57, 'danced': 58, 'dublin': 59, 'an': 60, 'put': 61, 'leg': 62, 'miss': 63, 'fainted': 64, 'from': 65, 'town': 66, 'athy': 67, 'one': 68, 'jeremy': 69, 'lanigan': 70, 'battered': 71, 'hadnt': 72, 'pound': 73, 'father': 74, 'died': 75, 'made': 76, 'man': 77, 'farm': 78, 'ten': 79, 'acres': 80, 'ground': 81, 'gave': 82, 'grand': 83, 'party': 84, 'who': 85, 'didnt': 86, 'forget': 87, 'come': 88, 'if': 89, 'youll': 90, 'but': 91, 'listen': 92, 'ill': 93, 'make': 94, 'your': 95, 'eyes': 96, 'glisten': 97, 'rows': 98, 'ructions': 99, 'be': 100, 'sure': 101, 'free': 102, 'invitation': 103, 'might': 104, 'ask': 105, 'minute': 106, 'both': 107, 'bees': 108, 'cask': 109, 'judy': 110, 'odaly': 111, 'little': 112, 'milliner': 113, 'wink': 114, 'give': 115, 'call': 116, 'arrived': 117, 'with': 118, 'peggy': 119, 'mcgilligan': 120, 'lashings': 121, 'punch': 122, 'wine': 123, 'ladies': 124, 'potatoes': 125, 'cakes': 126, 'bacon': 127, 'tea': 128, 'nolans': 129, 'dolans': 130, 'ogradys': 131, 'courting': 132, 'songs': 133, 'went': 134, 'plenty': 135, 'water': 136, 'harp': 137, 'once': 138, 'sounded': 139, 'taras': 140, 'hall': 141, 'sweet': 142, 'nelly': 143, 'gray': 144, 'rat': 145, 'catchers': 146, 'daughter': 147, 'singing': 148, 'together': 149, 'doing': 150, 'kinds': 151, 'nonsensical': 152, 'polkas': 153, 'room': 154, 'whirligig': 155, 'julia': 156, 'we': 157, 'banished': 158, 'nonsense': 159, 'twist': 160, 'reel': 161, 'jig': 162, 'ach': 163, 'mavrone': 164, 'how': 165, 'mad': 166, 'youd': 167, 'think': 168, 'ceiling': 169, 'would': 170, 'fall': 171, 'brooks': 172, 'academy': 173, 'learn': 174, 'nothing': 175, 'hearty': 176, 'around': 177, 'couples': 178, 'groups': 179, 'accident': 180, 'happened': 181, 'young': 182, 'terrance': 183, 'mccarthy': 184, 'right': 185, 'through': 186, 'finnertys': 187, 'hoops': 188, 'poor': 189, 'creature': 190, 'cried': 191, 'meelia': 192, 'murther': 193, 'called': 194, 'brothers': 195, 'gathered': 196, 'carmody': 197, 'swore': 198, 'hed': 199, 'go': 200, 'no': 201, 'further': 202, 'had': 203, 'satisfaction': 204, 'midst': 205, 'row': 206, 'kerrigan': 207, 'cheeks': 208, 'same': 209, 'red': 210, 'rose': 211, 'some': 212, 'lads': 213, 'declared': 214, 'painted': 215, 'took': 216, 'small': 217, 'drop': 218, 'too': 219, 'much': 220, 'suppose': 221, 'sweetheart': 222, 'ned': 223, 'morgan': 224, 'so': 225, 'powerful': 226, 'able': 227, 'saw': 228, 'fair': 229, 'colleen': 230, 'stretched': 231, 'by': 232, 'tore': 233, 'under': 234, 'table': 235, 'smashed': 236, 'chaneys': 237, 'oh': 238, 'twas': 239, 'then': 240, 'runctions': 241, 'lick': 242, 'big': 243, 'phelim': 244, 'mchugh': 245, 'replied': 246, 'introduction': 247, 'kicked': 248, 'terrible': 249, 'hullabaloo': 250, 'casey': 251, 'piper': 252, 'near': 253, 'being': 254, 'strangled': 255, 'squeezed': 256, 'pipes': 257, 'bellows': 258, 'chanters': 259, 'ribbons': 260, 'entangled': 261, 'end': 262}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9vH8Y59ajYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fbe5c15a-c784-4768-bf7b-f3022c5a4f27"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(xs, ys, epochs=500, verbose=1)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 453 samples\n",
            "Epoch 1/500\n",
            "453/453 [==============================] - 2s 4ms/sample - loss: 5.5700 - accuracy: 0.0155\n",
            "Epoch 2/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 5.5523 - accuracy: 0.0618\n",
            "Epoch 3/500\n",
            "453/453 [==============================] - 0s 282us/sample - loss: 5.5206 - accuracy: 0.0552\n",
            "Epoch 4/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 5.4188 - accuracy: 0.0508\n",
            "Epoch 5/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 5.1992 - accuracy: 0.0508\n",
            "Epoch 6/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 5.0796 - accuracy: 0.0508\n",
            "Epoch 7/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 5.0354 - accuracy: 0.0508\n",
            "Epoch 8/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 5.0008 - accuracy: 0.0508\n",
            "Epoch 9/500\n",
            "453/453 [==============================] - 0s 256us/sample - loss: 4.9700 - accuracy: 0.0552\n",
            "Epoch 10/500\n",
            "453/453 [==============================] - 0s 267us/sample - loss: 4.9448 - accuracy: 0.0574\n",
            "Epoch 11/500\n",
            "453/453 [==============================] - 0s 218us/sample - loss: 4.9038 - accuracy: 0.0662\n",
            "Epoch 12/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 4.8704 - accuracy: 0.0618\n",
            "Epoch 13/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 4.8335 - accuracy: 0.0684\n",
            "Epoch 14/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 4.7924 - accuracy: 0.0684\n",
            "Epoch 15/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 4.7519 - accuracy: 0.0684\n",
            "Epoch 16/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 4.7057 - accuracy: 0.0773\n",
            "Epoch 17/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 4.6589 - accuracy: 0.0684\n",
            "Epoch 18/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 4.6082 - accuracy: 0.0927\n",
            "Epoch 19/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 4.5574 - accuracy: 0.0817\n",
            "Epoch 20/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 4.5111 - accuracy: 0.0949\n",
            "Epoch 21/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 4.4645 - accuracy: 0.1060\n",
            "Epoch 22/500\n",
            "453/453 [==============================] - 0s 292us/sample - loss: 4.4179 - accuracy: 0.1148\n",
            "Epoch 23/500\n",
            "453/453 [==============================] - 0s 247us/sample - loss: 4.3767 - accuracy: 0.1038\n",
            "Epoch 24/500\n",
            "453/453 [==============================] - 0s 271us/sample - loss: 4.3275 - accuracy: 0.1170\n",
            "Epoch 25/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 4.2751 - accuracy: 0.1347\n",
            "Epoch 26/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 4.2238 - accuracy: 0.1391\n",
            "Epoch 27/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 4.1733 - accuracy: 0.1302\n",
            "Epoch 28/500\n",
            "453/453 [==============================] - 0s 223us/sample - loss: 4.1311 - accuracy: 0.1479\n",
            "Epoch 29/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 4.0779 - accuracy: 0.1501\n",
            "Epoch 30/500\n",
            "453/453 [==============================] - 0s 223us/sample - loss: 4.0360 - accuracy: 0.1678\n",
            "Epoch 31/500\n",
            "453/453 [==============================] - 0s 258us/sample - loss: 3.9885 - accuracy: 0.1656\n",
            "Epoch 32/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 3.9415 - accuracy: 0.1744\n",
            "Epoch 33/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 3.9081 - accuracy: 0.1788\n",
            "Epoch 34/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 3.8552 - accuracy: 0.2075\n",
            "Epoch 35/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 3.8061 - accuracy: 0.1965\n",
            "Epoch 36/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 3.7591 - accuracy: 0.2208\n",
            "Epoch 37/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 3.7218 - accuracy: 0.2185\n",
            "Epoch 38/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 3.6712 - accuracy: 0.2340\n",
            "Epoch 39/500\n",
            "453/453 [==============================] - 0s 222us/sample - loss: 3.6189 - accuracy: 0.2362\n",
            "Epoch 40/500\n",
            "453/453 [==============================] - 0s 221us/sample - loss: 3.5720 - accuracy: 0.2517\n",
            "Epoch 41/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 3.5244 - accuracy: 0.2605\n",
            "Epoch 42/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 3.4758 - accuracy: 0.2737\n",
            "Epoch 43/500\n",
            "453/453 [==============================] - 0s 247us/sample - loss: 3.4355 - accuracy: 0.2804\n",
            "Epoch 44/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 3.4043 - accuracy: 0.2870\n",
            "Epoch 45/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 3.3630 - accuracy: 0.3091\n",
            "Epoch 46/500\n",
            "453/453 [==============================] - 0s 223us/sample - loss: 3.3076 - accuracy: 0.3091\n",
            "Epoch 47/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 3.2668 - accuracy: 0.3223\n",
            "Epoch 48/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 3.2419 - accuracy: 0.3135\n",
            "Epoch 49/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 3.2046 - accuracy: 0.3355\n",
            "Epoch 50/500\n",
            "453/453 [==============================] - 0s 219us/sample - loss: 3.1556 - accuracy: 0.3377\n",
            "Epoch 51/500\n",
            "453/453 [==============================] - 0s 260us/sample - loss: 3.1199 - accuracy: 0.3444\n",
            "Epoch 52/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 3.0694 - accuracy: 0.3709\n",
            "Epoch 53/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 3.0373 - accuracy: 0.3709\n",
            "Epoch 54/500\n",
            "453/453 [==============================] - 0s 251us/sample - loss: 3.0007 - accuracy: 0.3819\n",
            "Epoch 55/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 2.9960 - accuracy: 0.3974\n",
            "Epoch 56/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 2.9493 - accuracy: 0.3929\n",
            "Epoch 57/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 2.8944 - accuracy: 0.4106\n",
            "Epoch 58/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 2.8887 - accuracy: 0.4018\n",
            "Epoch 59/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 2.8432 - accuracy: 0.4216\n",
            "Epoch 60/500\n",
            "453/453 [==============================] - 0s 221us/sample - loss: 2.8088 - accuracy: 0.4194\n",
            "Epoch 61/500\n",
            "453/453 [==============================] - 0s 262us/sample - loss: 2.7633 - accuracy: 0.4305\n",
            "Epoch 62/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 2.7273 - accuracy: 0.4371\n",
            "Epoch 63/500\n",
            "453/453 [==============================] - 0s 255us/sample - loss: 2.6964 - accuracy: 0.4636\n",
            "Epoch 64/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 2.6637 - accuracy: 0.4724\n",
            "Epoch 65/500\n",
            "453/453 [==============================] - 0s 223us/sample - loss: 2.6314 - accuracy: 0.4879\n",
            "Epoch 66/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 2.5927 - accuracy: 0.4879\n",
            "Epoch 67/500\n",
            "453/453 [==============================] - 0s 222us/sample - loss: 2.5615 - accuracy: 0.4967\n",
            "Epoch 68/500\n",
            "453/453 [==============================] - 0s 222us/sample - loss: 2.5305 - accuracy: 0.5011\n",
            "Epoch 69/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 2.5016 - accuracy: 0.5099\n",
            "Epoch 70/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 2.4718 - accuracy: 0.5121\n",
            "Epoch 71/500\n",
            "453/453 [==============================] - 0s 260us/sample - loss: 2.4521 - accuracy: 0.5210\n",
            "Epoch 72/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 2.4124 - accuracy: 0.5188\n",
            "Epoch 73/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 2.3914 - accuracy: 0.5188\n",
            "Epoch 74/500\n",
            "453/453 [==============================] - 0s 222us/sample - loss: 2.3608 - accuracy: 0.5276\n",
            "Epoch 75/500\n",
            "453/453 [==============================] - 0s 220us/sample - loss: 2.3406 - accuracy: 0.5276\n",
            "Epoch 76/500\n",
            "453/453 [==============================] - 0s 256us/sample - loss: 2.3105 - accuracy: 0.5408\n",
            "Epoch 77/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 2.2717 - accuracy: 0.5408\n",
            "Epoch 78/500\n",
            "453/453 [==============================] - 0s 218us/sample - loss: 2.2504 - accuracy: 0.5408\n",
            "Epoch 79/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 2.2433 - accuracy: 0.5475\n",
            "Epoch 80/500\n",
            "453/453 [==============================] - 0s 251us/sample - loss: 2.2176 - accuracy: 0.5740\n",
            "Epoch 81/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 2.1863 - accuracy: 0.5894\n",
            "Epoch 82/500\n",
            "453/453 [==============================] - 0s 261us/sample - loss: 2.1658 - accuracy: 0.5850\n",
            "Epoch 83/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 2.1566 - accuracy: 0.5784\n",
            "Epoch 84/500\n",
            "453/453 [==============================] - 0s 223us/sample - loss: 2.1226 - accuracy: 0.5872\n",
            "Epoch 85/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 2.0997 - accuracy: 0.6137\n",
            "Epoch 86/500\n",
            "453/453 [==============================] - 0s 215us/sample - loss: 2.0733 - accuracy: 0.6115\n",
            "Epoch 87/500\n",
            "453/453 [==============================] - 0s 217us/sample - loss: 2.0392 - accuracy: 0.6313\n",
            "Epoch 88/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 2.0107 - accuracy: 0.6225\n",
            "Epoch 89/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 1.9851 - accuracy: 0.6468\n",
            "Epoch 90/500\n",
            "453/453 [==============================] - 0s 277us/sample - loss: 1.9692 - accuracy: 0.6446\n",
            "Epoch 91/500\n",
            "453/453 [==============================] - 0s 258us/sample - loss: 1.9469 - accuracy: 0.6380\n",
            "Epoch 92/500\n",
            "453/453 [==============================] - 0s 278us/sample - loss: 1.9118 - accuracy: 0.6578\n",
            "Epoch 93/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 1.8895 - accuracy: 0.6667\n",
            "Epoch 94/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 1.8677 - accuracy: 0.6645\n",
            "Epoch 95/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 1.8425 - accuracy: 0.6843\n",
            "Epoch 96/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 1.8214 - accuracy: 0.6733\n",
            "Epoch 97/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 1.8025 - accuracy: 0.6843\n",
            "Epoch 98/500\n",
            "453/453 [==============================] - 0s 255us/sample - loss: 1.7834 - accuracy: 0.6865\n",
            "Epoch 99/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 1.7620 - accuracy: 0.6954\n",
            "Epoch 100/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 1.7351 - accuracy: 0.7086\n",
            "Epoch 101/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 1.7178 - accuracy: 0.7152\n",
            "Epoch 102/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 1.6980 - accuracy: 0.7020\n",
            "Epoch 103/500\n",
            "453/453 [==============================] - 0s 221us/sample - loss: 1.6778 - accuracy: 0.7108\n",
            "Epoch 104/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 1.6602 - accuracy: 0.7219\n",
            "Epoch 105/500\n",
            "453/453 [==============================] - 0s 221us/sample - loss: 1.6440 - accuracy: 0.7263\n",
            "Epoch 106/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 1.6300 - accuracy: 0.7351\n",
            "Epoch 107/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 1.6115 - accuracy: 0.7417\n",
            "Epoch 108/500\n",
            "453/453 [==============================] - 0s 285us/sample - loss: 1.5848 - accuracy: 0.7594\n",
            "Epoch 109/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 1.5653 - accuracy: 0.7550\n",
            "Epoch 110/500\n",
            "453/453 [==============================] - 0s 221us/sample - loss: 1.5734 - accuracy: 0.7417\n",
            "Epoch 111/500\n",
            "453/453 [==============================] - 0s 249us/sample - loss: 1.5857 - accuracy: 0.7483\n",
            "Epoch 112/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 1.5643 - accuracy: 0.7550\n",
            "Epoch 113/500\n",
            "453/453 [==============================] - 0s 220us/sample - loss: 1.5908 - accuracy: 0.7572\n",
            "Epoch 114/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 1.5619 - accuracy: 0.7417\n",
            "Epoch 115/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 1.5250 - accuracy: 0.7572\n",
            "Epoch 116/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 1.4806 - accuracy: 0.7682\n",
            "Epoch 117/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 1.4659 - accuracy: 0.7859\n",
            "Epoch 118/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 1.4437 - accuracy: 0.7682\n",
            "Epoch 119/500\n",
            "453/453 [==============================] - 0s 251us/sample - loss: 1.4158 - accuracy: 0.7947\n",
            "Epoch 120/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 1.4570 - accuracy: 0.7638\n",
            "Epoch 121/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 1.3998 - accuracy: 0.7969\n",
            "Epoch 122/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 1.3850 - accuracy: 0.7903\n",
            "Epoch 123/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 1.3627 - accuracy: 0.8035\n",
            "Epoch 124/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 1.3377 - accuracy: 0.8168\n",
            "Epoch 125/500\n",
            "453/453 [==============================] - 0s 216us/sample - loss: 1.3226 - accuracy: 0.8079\n",
            "Epoch 126/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 1.3046 - accuracy: 0.8146\n",
            "Epoch 127/500\n",
            "453/453 [==============================] - 0s 247us/sample - loss: 1.2925 - accuracy: 0.8212\n",
            "Epoch 128/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 1.2743 - accuracy: 0.8146\n",
            "Epoch 129/500\n",
            "453/453 [==============================] - 0s 253us/sample - loss: 1.2548 - accuracy: 0.8146\n",
            "Epoch 130/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 1.2409 - accuracy: 0.8344\n",
            "Epoch 131/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 1.2273 - accuracy: 0.8322\n",
            "Epoch 132/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 1.2178 - accuracy: 0.8322\n",
            "Epoch 133/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 1.2026 - accuracy: 0.8433\n",
            "Epoch 134/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 1.1870 - accuracy: 0.8389\n",
            "Epoch 135/500\n",
            "453/453 [==============================] - 0s 300us/sample - loss: 1.1735 - accuracy: 0.8455\n",
            "Epoch 136/500\n",
            "453/453 [==============================] - 0s 357us/sample - loss: 1.1597 - accuracy: 0.8433\n",
            "Epoch 137/500\n",
            "453/453 [==============================] - 0s 318us/sample - loss: 1.1456 - accuracy: 0.8455\n",
            "Epoch 138/500\n",
            "453/453 [==============================] - 0s 324us/sample - loss: 1.1334 - accuracy: 0.8455\n",
            "Epoch 139/500\n",
            "453/453 [==============================] - 0s 273us/sample - loss: 1.1210 - accuracy: 0.8543\n",
            "Epoch 140/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 1.1119 - accuracy: 0.8521\n",
            "Epoch 141/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 1.0984 - accuracy: 0.8631\n",
            "Epoch 142/500\n",
            "453/453 [==============================] - 0s 221us/sample - loss: 1.0852 - accuracy: 0.8653\n",
            "Epoch 143/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 1.0761 - accuracy: 0.8631\n",
            "Epoch 144/500\n",
            "453/453 [==============================] - 0s 256us/sample - loss: 1.0625 - accuracy: 0.8653\n",
            "Epoch 145/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 1.0504 - accuracy: 0.8720\n",
            "Epoch 146/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 1.0372 - accuracy: 0.8720\n",
            "Epoch 147/500\n",
            "453/453 [==============================] - 0s 251us/sample - loss: 1.0241 - accuracy: 0.8742\n",
            "Epoch 148/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 1.0138 - accuracy: 0.8742\n",
            "Epoch 149/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 1.0115 - accuracy: 0.8698\n",
            "Epoch 150/500\n",
            "453/453 [==============================] - 0s 253us/sample - loss: 0.9969 - accuracy: 0.8742\n",
            "Epoch 151/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 0.9839 - accuracy: 0.8786\n",
            "Epoch 152/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.9776 - accuracy: 0.8764\n",
            "Epoch 153/500\n",
            "453/453 [==============================] - 0s 254us/sample - loss: 0.9765 - accuracy: 0.8808\n",
            "Epoch 154/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 0.9589 - accuracy: 0.8808\n",
            "Epoch 155/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 0.9441 - accuracy: 0.8786\n",
            "Epoch 156/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.9315 - accuracy: 0.8852\n",
            "Epoch 157/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.9192 - accuracy: 0.8896\n",
            "Epoch 158/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.9112 - accuracy: 0.8852\n",
            "Epoch 159/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.9280 - accuracy: 0.8786\n",
            "Epoch 160/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.9511 - accuracy: 0.8609\n",
            "Epoch 161/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.9380 - accuracy: 0.8698\n",
            "Epoch 162/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.9502 - accuracy: 0.8653\n",
            "Epoch 163/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.9641 - accuracy: 0.8565\n",
            "Epoch 164/500\n",
            "453/453 [==============================] - 0s 219us/sample - loss: 0.9254 - accuracy: 0.8587\n",
            "Epoch 165/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.8872 - accuracy: 0.8764\n",
            "Epoch 166/500\n",
            "453/453 [==============================] - 0s 248us/sample - loss: 0.8562 - accuracy: 0.8896\n",
            "Epoch 167/500\n",
            "453/453 [==============================] - 0s 221us/sample - loss: 0.8439 - accuracy: 0.8830\n",
            "Epoch 168/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.8365 - accuracy: 0.8896\n",
            "Epoch 169/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 0.8196 - accuracy: 0.8940\n",
            "Epoch 170/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.8083 - accuracy: 0.8874\n",
            "Epoch 171/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 0.7991 - accuracy: 0.8918\n",
            "Epoch 172/500\n",
            "453/453 [==============================] - 0s 261us/sample - loss: 0.7899 - accuracy: 0.8896\n",
            "Epoch 173/500\n",
            "453/453 [==============================] - 0s 337us/sample - loss: 0.7856 - accuracy: 0.8918\n",
            "Epoch 174/500\n",
            "453/453 [==============================] - 0s 332us/sample - loss: 0.7732 - accuracy: 0.8896\n",
            "Epoch 175/500\n",
            "453/453 [==============================] - 0s 322us/sample - loss: 0.7658 - accuracy: 0.8896\n",
            "Epoch 176/500\n",
            "453/453 [==============================] - 0s 270us/sample - loss: 0.7561 - accuracy: 0.8962\n",
            "Epoch 177/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 0.7483 - accuracy: 0.8985\n",
            "Epoch 178/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 0.7406 - accuracy: 0.8962\n",
            "Epoch 179/500\n",
            "453/453 [==============================] - 0s 272us/sample - loss: 0.7322 - accuracy: 0.8962\n",
            "Epoch 180/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.7259 - accuracy: 0.9029\n",
            "Epoch 181/500\n",
            "453/453 [==============================] - 0s 223us/sample - loss: 0.7184 - accuracy: 0.9007\n",
            "Epoch 182/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 0.7097 - accuracy: 0.9029\n",
            "Epoch 183/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.7025 - accuracy: 0.9029\n",
            "Epoch 184/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.6982 - accuracy: 0.9051\n",
            "Epoch 185/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.6928 - accuracy: 0.9029\n",
            "Epoch 186/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 0.6849 - accuracy: 0.9051\n",
            "Epoch 187/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.6782 - accuracy: 0.9073\n",
            "Epoch 188/500\n",
            "453/453 [==============================] - 0s 247us/sample - loss: 0.6719 - accuracy: 0.9117\n",
            "Epoch 189/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.6690 - accuracy: 0.9139\n",
            "Epoch 190/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 0.6589 - accuracy: 0.9095\n",
            "Epoch 191/500\n",
            "453/453 [==============================] - 0s 219us/sample - loss: 0.6516 - accuracy: 0.9095\n",
            "Epoch 192/500\n",
            "453/453 [==============================] - 0s 222us/sample - loss: 0.6463 - accuracy: 0.9117\n",
            "Epoch 193/500\n",
            "453/453 [==============================] - 0s 221us/sample - loss: 0.6424 - accuracy: 0.9117\n",
            "Epoch 194/500\n",
            "453/453 [==============================] - 0s 247us/sample - loss: 0.6364 - accuracy: 0.9095\n",
            "Epoch 195/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.6368 - accuracy: 0.9051\n",
            "Epoch 196/500\n",
            "453/453 [==============================] - 0s 254us/sample - loss: 0.6367 - accuracy: 0.9161\n",
            "Epoch 197/500\n",
            "453/453 [==============================] - 0s 269us/sample - loss: 0.6311 - accuracy: 0.9095\n",
            "Epoch 198/500\n",
            "453/453 [==============================] - 0s 288us/sample - loss: 0.7123 - accuracy: 0.8940\n",
            "Epoch 199/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.7596 - accuracy: 0.8698\n",
            "Epoch 200/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.7336 - accuracy: 0.8786\n",
            "Epoch 201/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 0.6748 - accuracy: 0.9007\n",
            "Epoch 202/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.6414 - accuracy: 0.9117\n",
            "Epoch 203/500\n",
            "453/453 [==============================] - 0s 305us/sample - loss: 0.6407 - accuracy: 0.9073\n",
            "Epoch 204/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.6396 - accuracy: 0.9029\n",
            "Epoch 205/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 0.6213 - accuracy: 0.9139\n",
            "Epoch 206/500\n",
            "453/453 [==============================] - 0s 223us/sample - loss: 0.5978 - accuracy: 0.9183\n",
            "Epoch 207/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 0.5917 - accuracy: 0.9183\n",
            "Epoch 208/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.5828 - accuracy: 0.9205\n",
            "Epoch 209/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.5751 - accuracy: 0.9249\n",
            "Epoch 210/500\n",
            "453/453 [==============================] - 0s 222us/sample - loss: 0.5820 - accuracy: 0.9183\n",
            "Epoch 211/500\n",
            "453/453 [==============================] - 0s 221us/sample - loss: 0.5607 - accuracy: 0.9272\n",
            "Epoch 212/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.5596 - accuracy: 0.9272\n",
            "Epoch 213/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 0.5464 - accuracy: 0.9316\n",
            "Epoch 214/500\n",
            "453/453 [==============================] - 0s 220us/sample - loss: 0.5410 - accuracy: 0.9338\n",
            "Epoch 215/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 0.5383 - accuracy: 0.9360\n",
            "Epoch 216/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 0.5261 - accuracy: 0.9360\n",
            "Epoch 217/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.5209 - accuracy: 0.9448\n",
            "Epoch 218/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.5154 - accuracy: 0.9404\n",
            "Epoch 219/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.5117 - accuracy: 0.9426\n",
            "Epoch 220/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.5068 - accuracy: 0.9338\n",
            "Epoch 221/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.5067 - accuracy: 0.9448\n",
            "Epoch 222/500\n",
            "453/453 [==============================] - 0s 254us/sample - loss: 0.4976 - accuracy: 0.9382\n",
            "Epoch 223/500\n",
            "453/453 [==============================] - 0s 294us/sample - loss: 0.4943 - accuracy: 0.9338\n",
            "Epoch 224/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.4905 - accuracy: 0.9360\n",
            "Epoch 225/500\n",
            "453/453 [==============================] - 0s 251us/sample - loss: 0.4836 - accuracy: 0.9404\n",
            "Epoch 226/500\n",
            "453/453 [==============================] - 0s 277us/sample - loss: 0.4788 - accuracy: 0.9382\n",
            "Epoch 227/500\n",
            "453/453 [==============================] - 0s 220us/sample - loss: 0.4743 - accuracy: 0.9426\n",
            "Epoch 228/500\n",
            "453/453 [==============================] - 0s 217us/sample - loss: 0.4705 - accuracy: 0.9404\n",
            "Epoch 229/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.4668 - accuracy: 0.9426\n",
            "Epoch 230/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 0.4637 - accuracy: 0.9404\n",
            "Epoch 231/500\n",
            "453/453 [==============================] - 0s 217us/sample - loss: 0.4589 - accuracy: 0.9404\n",
            "Epoch 232/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 0.4548 - accuracy: 0.9426\n",
            "Epoch 233/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.4515 - accuracy: 0.9448\n",
            "Epoch 234/500\n",
            "453/453 [==============================] - 0s 248us/sample - loss: 0.4492 - accuracy: 0.9448\n",
            "Epoch 235/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 0.4440 - accuracy: 0.9426\n",
            "Epoch 236/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.4400 - accuracy: 0.9448\n",
            "Epoch 237/500\n",
            "453/453 [==============================] - 0s 220us/sample - loss: 0.4370 - accuracy: 0.9470\n",
            "Epoch 238/500\n",
            "453/453 [==============================] - 0s 215us/sample - loss: 0.4324 - accuracy: 0.9448\n",
            "Epoch 239/500\n",
            "453/453 [==============================] - 0s 222us/sample - loss: 0.4294 - accuracy: 0.9448\n",
            "Epoch 240/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.4263 - accuracy: 0.9448\n",
            "Epoch 241/500\n",
            "453/453 [==============================] - 0s 222us/sample - loss: 0.4221 - accuracy: 0.9448\n",
            "Epoch 242/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 0.4198 - accuracy: 0.9492\n",
            "Epoch 243/500\n",
            "453/453 [==============================] - 0s 255us/sample - loss: 0.4164 - accuracy: 0.9448\n",
            "Epoch 244/500\n",
            "453/453 [==============================] - 0s 263us/sample - loss: 0.4126 - accuracy: 0.9470\n",
            "Epoch 245/500\n",
            "453/453 [==============================] - 0s 251us/sample - loss: 0.4091 - accuracy: 0.9448\n",
            "Epoch 246/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 0.4055 - accuracy: 0.9448\n",
            "Epoch 247/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.4016 - accuracy: 0.9448\n",
            "Epoch 248/500\n",
            "453/453 [==============================] - 0s 262us/sample - loss: 0.3993 - accuracy: 0.9448\n",
            "Epoch 249/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 0.3963 - accuracy: 0.9448\n",
            "Epoch 250/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 0.3935 - accuracy: 0.9404\n",
            "Epoch 251/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 0.3896 - accuracy: 0.9470\n",
            "Epoch 252/500\n",
            "453/453 [==============================] - 0s 248us/sample - loss: 0.3865 - accuracy: 0.9448\n",
            "Epoch 253/500\n",
            "453/453 [==============================] - 0s 261us/sample - loss: 0.3835 - accuracy: 0.9448\n",
            "Epoch 254/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.3805 - accuracy: 0.9470\n",
            "Epoch 255/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.3790 - accuracy: 0.9514\n",
            "Epoch 256/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.3764 - accuracy: 0.9448\n",
            "Epoch 257/500\n",
            "453/453 [==============================] - 0s 221us/sample - loss: 0.3734 - accuracy: 0.9426\n",
            "Epoch 258/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.3705 - accuracy: 0.9470\n",
            "Epoch 259/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.3674 - accuracy: 0.9470\n",
            "Epoch 260/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 0.3667 - accuracy: 0.9470\n",
            "Epoch 261/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 0.3650 - accuracy: 0.9404\n",
            "Epoch 262/500\n",
            "453/453 [==============================] - 0s 275us/sample - loss: 0.3611 - accuracy: 0.9492\n",
            "Epoch 263/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.3625 - accuracy: 0.9470\n",
            "Epoch 264/500\n",
            "453/453 [==============================] - 0s 254us/sample - loss: 0.3592 - accuracy: 0.9470\n",
            "Epoch 265/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.3560 - accuracy: 0.9470\n",
            "Epoch 266/500\n",
            "453/453 [==============================] - 0s 223us/sample - loss: 0.3536 - accuracy: 0.9470\n",
            "Epoch 267/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 0.3510 - accuracy: 0.9470\n",
            "Epoch 268/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.3490 - accuracy: 0.9470\n",
            "Epoch 269/500\n",
            "453/453 [==============================] - 0s 223us/sample - loss: 0.3486 - accuracy: 0.9492\n",
            "Epoch 270/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 0.3436 - accuracy: 0.9404\n",
            "Epoch 271/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 0.3404 - accuracy: 0.9470\n",
            "Epoch 272/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.3424 - accuracy: 0.9470\n",
            "Epoch 273/500\n",
            "453/453 [==============================] - 0s 253us/sample - loss: 0.4580 - accuracy: 0.9117\n",
            "Epoch 274/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 0.4480 - accuracy: 0.9139\n",
            "Epoch 275/500\n",
            "453/453 [==============================] - 0s 223us/sample - loss: 0.4178 - accuracy: 0.9227\n",
            "Epoch 276/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 0.4488 - accuracy: 0.9139\n",
            "Epoch 277/500\n",
            "453/453 [==============================] - 0s 218us/sample - loss: 0.5357 - accuracy: 0.8940\n",
            "Epoch 278/500\n",
            "453/453 [==============================] - 0s 222us/sample - loss: 0.5914 - accuracy: 0.8830\n",
            "Epoch 279/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 0.4742 - accuracy: 0.9007\n",
            "Epoch 280/500\n",
            "453/453 [==============================] - 0s 222us/sample - loss: 0.4283 - accuracy: 0.9249\n",
            "Epoch 281/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 0.3892 - accuracy: 0.9382\n",
            "Epoch 282/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 0.3846 - accuracy: 0.9338\n",
            "Epoch 283/500\n",
            "453/453 [==============================] - 0s 287us/sample - loss: 0.3714 - accuracy: 0.9338\n",
            "Epoch 284/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 0.3535 - accuracy: 0.9426\n",
            "Epoch 285/500\n",
            "453/453 [==============================] - 0s 263us/sample - loss: 0.3439 - accuracy: 0.9426\n",
            "Epoch 286/500\n",
            "453/453 [==============================] - 0s 223us/sample - loss: 0.3330 - accuracy: 0.9448\n",
            "Epoch 287/500\n",
            "453/453 [==============================] - 0s 218us/sample - loss: 0.3296 - accuracy: 0.9470\n",
            "Epoch 288/500\n",
            "453/453 [==============================] - 0s 256us/sample - loss: 0.3261 - accuracy: 0.9426\n",
            "Epoch 289/500\n",
            "453/453 [==============================] - 0s 260us/sample - loss: 0.3177 - accuracy: 0.9470\n",
            "Epoch 290/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.3161 - accuracy: 0.9426\n",
            "Epoch 291/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.3087 - accuracy: 0.9426\n",
            "Epoch 292/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.3088 - accuracy: 0.9426\n",
            "Epoch 293/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 0.2976 - accuracy: 0.9492\n",
            "Epoch 294/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.2942 - accuracy: 0.9514\n",
            "Epoch 295/500\n",
            "453/453 [==============================] - 0s 290us/sample - loss: 0.2961 - accuracy: 0.9426\n",
            "Epoch 296/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 0.2917 - accuracy: 0.9492\n",
            "Epoch 297/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.2869 - accuracy: 0.9514\n",
            "Epoch 298/500\n",
            "453/453 [==============================] - 0s 283us/sample - loss: 0.2850 - accuracy: 0.9492\n",
            "Epoch 299/500\n",
            "453/453 [==============================] - 0s 247us/sample - loss: 0.2830 - accuracy: 0.9514\n",
            "Epoch 300/500\n",
            "453/453 [==============================] - 0s 276us/sample - loss: 0.2793 - accuracy: 0.9514\n",
            "Epoch 301/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.2768 - accuracy: 0.9536\n",
            "Epoch 302/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 0.2749 - accuracy: 0.9492\n",
            "Epoch 303/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.2747 - accuracy: 0.9514\n",
            "Epoch 304/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.2722 - accuracy: 0.9536\n",
            "Epoch 305/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.2728 - accuracy: 0.9448\n",
            "Epoch 306/500\n",
            "453/453 [==============================] - 0s 251us/sample - loss: 0.2696 - accuracy: 0.9492\n",
            "Epoch 307/500\n",
            "453/453 [==============================] - 0s 247us/sample - loss: 0.2676 - accuracy: 0.9492\n",
            "Epoch 308/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 0.2640 - accuracy: 0.9492\n",
            "Epoch 309/500\n",
            "453/453 [==============================] - 0s 223us/sample - loss: 0.2634 - accuracy: 0.9514\n",
            "Epoch 310/500\n",
            "453/453 [==============================] - 0s 260us/sample - loss: 0.2621 - accuracy: 0.9536\n",
            "Epoch 311/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.2602 - accuracy: 0.9536\n",
            "Epoch 312/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 0.2583 - accuracy: 0.9470\n",
            "Epoch 313/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.2562 - accuracy: 0.9514\n",
            "Epoch 314/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.2548 - accuracy: 0.9514\n",
            "Epoch 315/500\n",
            "453/453 [==============================] - 0s 219us/sample - loss: 0.2527 - accuracy: 0.9492\n",
            "Epoch 316/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.2520 - accuracy: 0.9536\n",
            "Epoch 317/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 0.2493 - accuracy: 0.9492\n",
            "Epoch 318/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.2478 - accuracy: 0.9492\n",
            "Epoch 319/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 0.2465 - accuracy: 0.9514\n",
            "Epoch 320/500\n",
            "453/453 [==============================] - 0s 278us/sample - loss: 0.2444 - accuracy: 0.9536\n",
            "Epoch 321/500\n",
            "453/453 [==============================] - 0s 248us/sample - loss: 0.2437 - accuracy: 0.9536\n",
            "Epoch 322/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 0.2416 - accuracy: 0.9492\n",
            "Epoch 323/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.2410 - accuracy: 0.9514\n",
            "Epoch 324/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 0.2375 - accuracy: 0.9514\n",
            "Epoch 325/500\n",
            "453/453 [==============================] - 0s 254us/sample - loss: 0.2364 - accuracy: 0.9492\n",
            "Epoch 326/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 0.2355 - accuracy: 0.9536\n",
            "Epoch 327/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.2338 - accuracy: 0.9492\n",
            "Epoch 328/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.2365 - accuracy: 0.9514\n",
            "Epoch 329/500\n",
            "453/453 [==============================] - 0s 257us/sample - loss: 0.2312 - accuracy: 0.9536\n",
            "Epoch 330/500\n",
            "453/453 [==============================] - 0s 218us/sample - loss: 0.2296 - accuracy: 0.9492\n",
            "Epoch 331/500\n",
            "453/453 [==============================] - 0s 272us/sample - loss: 0.2277 - accuracy: 0.9536\n",
            "Epoch 332/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 0.2269 - accuracy: 0.9536\n",
            "Epoch 333/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 0.2257 - accuracy: 0.9492\n",
            "Epoch 334/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.2251 - accuracy: 0.9492\n",
            "Epoch 335/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.2241 - accuracy: 0.9514\n",
            "Epoch 336/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.2237 - accuracy: 0.9514\n",
            "Epoch 337/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.2210 - accuracy: 0.9514\n",
            "Epoch 338/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 0.2210 - accuracy: 0.9514\n",
            "Epoch 339/500\n",
            "453/453 [==============================] - 0s 249us/sample - loss: 0.2186 - accuracy: 0.9536\n",
            "Epoch 340/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.2168 - accuracy: 0.9536\n",
            "Epoch 341/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.2159 - accuracy: 0.9492\n",
            "Epoch 342/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 0.2162 - accuracy: 0.9514\n",
            "Epoch 343/500\n",
            "453/453 [==============================] - 0s 222us/sample - loss: 0.2193 - accuracy: 0.9492\n",
            "Epoch 344/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.2420 - accuracy: 0.9382\n",
            "Epoch 345/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.2399 - accuracy: 0.9448\n",
            "Epoch 346/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.2390 - accuracy: 0.9426\n",
            "Epoch 347/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.2334 - accuracy: 0.9448\n",
            "Epoch 348/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 0.2390 - accuracy: 0.9382\n",
            "Epoch 349/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 0.2295 - accuracy: 0.9448\n",
            "Epoch 350/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.2238 - accuracy: 0.9470\n",
            "Epoch 351/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 0.2209 - accuracy: 0.9448\n",
            "Epoch 352/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 0.2176 - accuracy: 0.9470\n",
            "Epoch 353/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.2146 - accuracy: 0.9470\n",
            "Epoch 354/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 0.2133 - accuracy: 0.9448\n",
            "Epoch 355/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.2119 - accuracy: 0.9492\n",
            "Epoch 356/500\n",
            "453/453 [==============================] - 0s 248us/sample - loss: 0.2100 - accuracy: 0.9536\n",
            "Epoch 357/500\n",
            "453/453 [==============================] - 0s 315us/sample - loss: 0.2096 - accuracy: 0.9536\n",
            "Epoch 358/500\n",
            "453/453 [==============================] - 0s 253us/sample - loss: 0.2075 - accuracy: 0.9514\n",
            "Epoch 359/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.2088 - accuracy: 0.9492\n",
            "Epoch 360/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.2085 - accuracy: 0.9514\n",
            "Epoch 361/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 0.2043 - accuracy: 0.9536\n",
            "Epoch 362/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 0.2024 - accuracy: 0.9536\n",
            "Epoch 363/500\n",
            "453/453 [==============================] - 0s 255us/sample - loss: 0.2017 - accuracy: 0.9536\n",
            "Epoch 364/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.2003 - accuracy: 0.9536\n",
            "Epoch 365/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.2003 - accuracy: 0.9536\n",
            "Epoch 366/500\n",
            "453/453 [==============================] - 0s 218us/sample - loss: 0.1982 - accuracy: 0.9536\n",
            "Epoch 367/500\n",
            "453/453 [==============================] - 0s 248us/sample - loss: 0.1988 - accuracy: 0.9514\n",
            "Epoch 368/500\n",
            "453/453 [==============================] - 0s 255us/sample - loss: 0.1977 - accuracy: 0.9558\n",
            "Epoch 369/500\n",
            "453/453 [==============================] - 0s 217us/sample - loss: 0.1952 - accuracy: 0.9492\n",
            "Epoch 370/500\n",
            "453/453 [==============================] - 0s 253us/sample - loss: 0.1944 - accuracy: 0.9514\n",
            "Epoch 371/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.1941 - accuracy: 0.9492\n",
            "Epoch 372/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.1917 - accuracy: 0.9536\n",
            "Epoch 373/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.1923 - accuracy: 0.9492\n",
            "Epoch 374/500\n",
            "453/453 [==============================] - 0s 258us/sample - loss: 0.1889 - accuracy: 0.9536\n",
            "Epoch 375/500\n",
            "453/453 [==============================] - 0s 212us/sample - loss: 0.1895 - accuracy: 0.9492\n",
            "Epoch 376/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.1883 - accuracy: 0.9514\n",
            "Epoch 377/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 0.1866 - accuracy: 0.9492\n",
            "Epoch 378/500\n",
            "453/453 [==============================] - 0s 221us/sample - loss: 0.1858 - accuracy: 0.9514\n",
            "Epoch 379/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.1857 - accuracy: 0.9492\n",
            "Epoch 380/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 0.1844 - accuracy: 0.9492\n",
            "Epoch 381/500\n",
            "453/453 [==============================] - 0s 258us/sample - loss: 0.1849 - accuracy: 0.9492\n",
            "Epoch 382/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 0.1861 - accuracy: 0.9470\n",
            "Epoch 383/500\n",
            "453/453 [==============================] - 0s 221us/sample - loss: 0.1826 - accuracy: 0.9536\n",
            "Epoch 384/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.1823 - accuracy: 0.9492\n",
            "Epoch 385/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 0.1805 - accuracy: 0.9492\n",
            "Epoch 386/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 0.1799 - accuracy: 0.9470\n",
            "Epoch 387/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.1784 - accuracy: 0.9492\n",
            "Epoch 388/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.1782 - accuracy: 0.9448\n",
            "Epoch 389/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.1781 - accuracy: 0.9470\n",
            "Epoch 390/500\n",
            "453/453 [==============================] - 0s 258us/sample - loss: 0.1792 - accuracy: 0.9492\n",
            "Epoch 391/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.1769 - accuracy: 0.9536\n",
            "Epoch 392/500\n",
            "453/453 [==============================] - 0s 219us/sample - loss: 0.1749 - accuracy: 0.9536\n",
            "Epoch 393/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.1745 - accuracy: 0.9514\n",
            "Epoch 394/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 0.1734 - accuracy: 0.9492\n",
            "Epoch 395/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 0.1737 - accuracy: 0.9536\n",
            "Epoch 396/500\n",
            "453/453 [==============================] - 0s 218us/sample - loss: 0.1725 - accuracy: 0.9536\n",
            "Epoch 397/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.1723 - accuracy: 0.9470\n",
            "Epoch 398/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 0.1730 - accuracy: 0.9514\n",
            "Epoch 399/500\n",
            "453/453 [==============================] - 0s 253us/sample - loss: 0.1708 - accuracy: 0.9448\n",
            "Epoch 400/500\n",
            "453/453 [==============================] - 0s 256us/sample - loss: 0.1736 - accuracy: 0.9514\n",
            "Epoch 401/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.1782 - accuracy: 0.9492\n",
            "Epoch 402/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 0.1715 - accuracy: 0.9492\n",
            "Epoch 403/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.1712 - accuracy: 0.9514\n",
            "Epoch 404/500\n",
            "453/453 [==============================] - 0s 247us/sample - loss: 0.1710 - accuracy: 0.9536\n",
            "Epoch 405/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 0.1709 - accuracy: 0.9470\n",
            "Epoch 406/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.1670 - accuracy: 0.9514\n",
            "Epoch 407/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 0.1669 - accuracy: 0.9514\n",
            "Epoch 408/500\n",
            "453/453 [==============================] - 0s 350us/sample - loss: 0.1707 - accuracy: 0.9470\n",
            "Epoch 409/500\n",
            "453/453 [==============================] - 0s 327us/sample - loss: 0.1685 - accuracy: 0.9404\n",
            "Epoch 410/500\n",
            "453/453 [==============================] - 0s 309us/sample - loss: 0.1661 - accuracy: 0.9404\n",
            "Epoch 411/500\n",
            "453/453 [==============================] - 0s 300us/sample - loss: 0.1640 - accuracy: 0.9514\n",
            "Epoch 412/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.1627 - accuracy: 0.9492\n",
            "Epoch 413/500\n",
            "453/453 [==============================] - 0s 220us/sample - loss: 0.1614 - accuracy: 0.9470\n",
            "Epoch 414/500\n",
            "453/453 [==============================] - 0s 220us/sample - loss: 0.1613 - accuracy: 0.9448\n",
            "Epoch 415/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 0.1604 - accuracy: 0.9514\n",
            "Epoch 416/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.1594 - accuracy: 0.9536\n",
            "Epoch 417/500\n",
            "453/453 [==============================] - 0s 261us/sample - loss: 0.1591 - accuracy: 0.9514\n",
            "Epoch 418/500\n",
            "453/453 [==============================] - 0s 223us/sample - loss: 0.1580 - accuracy: 0.9536\n",
            "Epoch 419/500\n",
            "453/453 [==============================] - 0s 221us/sample - loss: 0.1576 - accuracy: 0.9514\n",
            "Epoch 420/500\n",
            "453/453 [==============================] - 0s 221us/sample - loss: 0.1573 - accuracy: 0.9492\n",
            "Epoch 421/500\n",
            "453/453 [==============================] - 0s 220us/sample - loss: 0.1572 - accuracy: 0.9470\n",
            "Epoch 422/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.1561 - accuracy: 0.9492\n",
            "Epoch 423/500\n",
            "453/453 [==============================] - 0s 278us/sample - loss: 0.1556 - accuracy: 0.9492\n",
            "Epoch 424/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 0.1559 - accuracy: 0.9492\n",
            "Epoch 425/500\n",
            "453/453 [==============================] - 0s 256us/sample - loss: 0.1561 - accuracy: 0.9492\n",
            "Epoch 426/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 0.1552 - accuracy: 0.9448\n",
            "Epoch 427/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.1542 - accuracy: 0.9470\n",
            "Epoch 428/500\n",
            "453/453 [==============================] - 0s 223us/sample - loss: 0.1548 - accuracy: 0.9470\n",
            "Epoch 429/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.1539 - accuracy: 0.9492\n",
            "Epoch 430/500\n",
            "453/453 [==============================] - 0s 222us/sample - loss: 0.1527 - accuracy: 0.9492\n",
            "Epoch 431/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.1517 - accuracy: 0.9470\n",
            "Epoch 432/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.1535 - accuracy: 0.9514\n",
            "Epoch 433/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 0.1612 - accuracy: 0.9470\n",
            "Epoch 434/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.1726 - accuracy: 0.9470\n",
            "Epoch 435/500\n",
            "453/453 [==============================] - 0s 253us/sample - loss: 0.1877 - accuracy: 0.9404\n",
            "Epoch 436/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.2581 - accuracy: 0.9316\n",
            "Epoch 437/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.2536 - accuracy: 0.9294\n",
            "Epoch 438/500\n",
            "453/453 [==============================] - 0s 223us/sample - loss: 0.2302 - accuracy: 0.9426\n",
            "Epoch 439/500\n",
            "453/453 [==============================] - 0s 220us/sample - loss: 0.2435 - accuracy: 0.9316\n",
            "Epoch 440/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 0.2617 - accuracy: 0.9249\n",
            "Epoch 441/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.2130 - accuracy: 0.9404\n",
            "Epoch 442/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.2001 - accuracy: 0.9492\n",
            "Epoch 443/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 0.2594 - accuracy: 0.9272\n",
            "Epoch 444/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.2467 - accuracy: 0.9360\n",
            "Epoch 445/500\n",
            "453/453 [==============================] - 0s 249us/sample - loss: 0.2096 - accuracy: 0.9360\n",
            "Epoch 446/500\n",
            "453/453 [==============================] - 0s 268us/sample - loss: 0.1928 - accuracy: 0.9426\n",
            "Epoch 447/500\n",
            "453/453 [==============================] - 0s 217us/sample - loss: 0.1810 - accuracy: 0.9426\n",
            "Epoch 448/500\n",
            "453/453 [==============================] - 0s 278us/sample - loss: 0.1750 - accuracy: 0.9492\n",
            "Epoch 449/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 0.1694 - accuracy: 0.9470\n",
            "Epoch 450/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.1616 - accuracy: 0.9514\n",
            "Epoch 451/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 0.1582 - accuracy: 0.9514\n",
            "Epoch 452/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.1553 - accuracy: 0.9492\n",
            "Epoch 453/500\n",
            "453/453 [==============================] - 0s 274us/sample - loss: 0.1534 - accuracy: 0.9492\n",
            "Epoch 454/500\n",
            "453/453 [==============================] - 0s 283us/sample - loss: 0.1543 - accuracy: 0.9558\n",
            "Epoch 455/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.1512 - accuracy: 0.9514\n",
            "Epoch 456/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 0.1511 - accuracy: 0.9448\n",
            "Epoch 457/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.1482 - accuracy: 0.9492\n",
            "Epoch 458/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 0.1483 - accuracy: 0.9470\n",
            "Epoch 459/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.1477 - accuracy: 0.9514\n",
            "Epoch 460/500\n",
            "453/453 [==============================] - 0s 220us/sample - loss: 0.1468 - accuracy: 0.9448\n",
            "Epoch 461/500\n",
            "453/453 [==============================] - 0s 222us/sample - loss: 0.1460 - accuracy: 0.9492\n",
            "Epoch 462/500\n",
            "453/453 [==============================] - 0s 260us/sample - loss: 0.1504 - accuracy: 0.9514\n",
            "Epoch 463/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.1477 - accuracy: 0.9536\n",
            "Epoch 464/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 0.1455 - accuracy: 0.9448\n",
            "Epoch 465/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.1446 - accuracy: 0.9514\n",
            "Epoch 466/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 0.1424 - accuracy: 0.9492\n",
            "Epoch 467/500\n",
            "453/453 [==============================] - 0s 222us/sample - loss: 0.1430 - accuracy: 0.9492\n",
            "Epoch 468/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.1422 - accuracy: 0.9514\n",
            "Epoch 469/500\n",
            "453/453 [==============================] - 0s 275us/sample - loss: 0.1422 - accuracy: 0.9470\n",
            "Epoch 470/500\n",
            "453/453 [==============================] - 0s 260us/sample - loss: 0.1418 - accuracy: 0.9492\n",
            "Epoch 471/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.1406 - accuracy: 0.9492\n",
            "Epoch 472/500\n",
            "453/453 [==============================] - 0s 277us/sample - loss: 0.1402 - accuracy: 0.9492\n",
            "Epoch 473/500\n",
            "453/453 [==============================] - 0s 265us/sample - loss: 0.1393 - accuracy: 0.9492\n",
            "Epoch 474/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 0.1393 - accuracy: 0.9492\n",
            "Epoch 475/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.1394 - accuracy: 0.9514\n",
            "Epoch 476/500\n",
            "453/453 [==============================] - 0s 222us/sample - loss: 0.1383 - accuracy: 0.9536\n",
            "Epoch 477/500\n",
            "453/453 [==============================] - 0s 220us/sample - loss: 0.1370 - accuracy: 0.9492\n",
            "Epoch 478/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.1366 - accuracy: 0.9492\n",
            "Epoch 479/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.1370 - accuracy: 0.9514\n",
            "Epoch 480/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 0.1362 - accuracy: 0.9514\n",
            "Epoch 481/500\n",
            "453/453 [==============================] - 0s 221us/sample - loss: 0.1362 - accuracy: 0.9536\n",
            "Epoch 482/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.1365 - accuracy: 0.9536\n",
            "Epoch 483/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.1354 - accuracy: 0.9514\n",
            "Epoch 484/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.1349 - accuracy: 0.9514\n",
            "Epoch 485/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.1342 - accuracy: 0.9492\n",
            "Epoch 486/500\n",
            "453/453 [==============================] - 0s 282us/sample - loss: 0.1337 - accuracy: 0.9492\n",
            "Epoch 487/500\n",
            "453/453 [==============================] - 0s 336us/sample - loss: 0.1330 - accuracy: 0.9514\n",
            "Epoch 488/500\n",
            "453/453 [==============================] - 0s 333us/sample - loss: 0.1329 - accuracy: 0.9492\n",
            "Epoch 489/500\n",
            "453/453 [==============================] - 0s 306us/sample - loss: 0.1329 - accuracy: 0.9470\n",
            "Epoch 490/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 0.1328 - accuracy: 0.9514\n",
            "Epoch 491/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.1307 - accuracy: 0.9492\n",
            "Epoch 492/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 0.1320 - accuracy: 0.9514\n",
            "Epoch 493/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.1310 - accuracy: 0.9470\n",
            "Epoch 494/500\n",
            "453/453 [==============================] - 0s 223us/sample - loss: 0.1304 - accuracy: 0.9404\n",
            "Epoch 495/500\n",
            "453/453 [==============================] - 0s 312us/sample - loss: 0.1303 - accuracy: 0.9426\n",
            "Epoch 496/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.1305 - accuracy: 0.9492\n",
            "Epoch 497/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 0.1322 - accuracy: 0.9470\n",
            "Epoch 498/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 0.1313 - accuracy: 0.9448\n",
            "Epoch 499/500\n",
            "453/453 [==============================] - 0s 255us/sample - loss: 0.1306 - accuracy: 0.9514\n",
            "Epoch 500/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.1296 - accuracy: 0.9470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YXGelKThoTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poeprYK8h-c7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "d72b995d-6fce-4c7c-b31f-27029cdaeb6e"
      },
      "source": [
        "plot_graphs(history, 'accuracy')\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBCIiMU6JQEQkxv1/bAD2gDUR8LcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vc6PHgxa6Hm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "56e33e5f-ff65-404f-c775-a6c88602d318"
      },
      "source": [
        "# Now predict the next word\n",
        "\n",
        "seed_text = \"Laurence went to dublin\" # feed the model seed data\n",
        "next_words = 100 # ask the next 100 words\n",
        "\n",
        "# for each of the next 100 words\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0] # create a token list\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre') # pad token list\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0) # pass generated token list to model to predict classes\n",
        "\toutput_word = \"\" \n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word # get predicted word if the index matches the predicted class\n",
        "\t\t\tbreak\n",
        "\tprint(f'Predicted Class: {predicted}. Word:  {output_word}')\n",
        "\tseed_text += \" \" + output_word # add output word to the seed text to then predict the next work\n",
        "print(seed_text)\n",
        "\n",
        "# note probabilities of the next words decrease as time goes on, so it gets more gobbledey gook"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Class: [2]. Word:  the\n",
            "Predicted Class: [41]. Word:  left\n",
            "Predicted Class: [62]. Word:  leg\n",
            "Predicted Class: [65]. Word:  from\n",
            "Predicted Class: [234]. Word:  under\n",
            "Predicted Class: [2]. Word:  the\n",
            "Predicted Class: [244]. Word:  phelim\n",
            "Predicted Class: [245]. Word:  mchugh\n",
            "Predicted Class: [141]. Word:  hall\n",
            "Predicted Class: [38]. Word:  steps\n",
            "Predicted Class: [38]. Word:  steps\n",
            "Predicted Class: [10]. Word:  ball\n",
            "Predicted Class: [7]. Word:  for\n",
            "Predicted Class: [9]. Word:  lanigans\n",
            "Predicted Class: [10]. Word:  ball\n",
            "Predicted Class: [10]. Word:  ball\n",
            "Predicted Class: [13]. Word:  to\n",
            "Predicted Class: [2]. Word:  the\n",
            "Predicted Class: [124]. Word:  ladies\n",
            "Predicted Class: [124]. Word:  ladies\n",
            "Predicted Class: [141]. Word:  hall\n",
            "Predicted Class: [3]. Word:  a\n",
            "Predicted Class: [116]. Word:  call\n",
            "Predicted Class: [116]. Word:  call\n",
            "Predicted Class: [96]. Word:  eyes\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [151]. Word:  kinds\n",
            "Predicted Class: [188]. Word:  hoops\n",
            "Predicted Class: [258]. Word:  bellows\n",
            "Predicted Class: [120]. Word:  mcgilligan\n",
            "Predicted Class: [2]. Word:  the\n",
            "Predicted Class: [202]. Word:  further\n",
            "Predicted Class: [202]. Word:  further\n",
            "Predicted Class: [141]. Word:  hall\n",
            "Predicted Class: [38]. Word:  steps\n",
            "Predicted Class: [38]. Word:  steps\n",
            "Predicted Class: [10]. Word:  ball\n",
            "Predicted Class: [12]. Word:  at\n",
            "Predicted Class: [18]. Word:  as\n",
            "Predicted Class: [135]. Word:  plenty\n",
            "Predicted Class: [18]. Word:  as\n",
            "Predicted Class: [210]. Word:  red\n",
            "Predicted Class: [18]. Word:  as\n",
            "Predicted Class: [18]. Word:  as\n",
            "Predicted Class: [210]. Word:  red\n",
            "Predicted Class: [18]. Word:  as\n",
            "Predicted Class: [3]. Word:  a\n",
            "Predicted Class: [211]. Word:  rose\n",
            "Predicted Class: [211]. Word:  rose\n",
            "Predicted Class: [210]. Word:  red\n",
            "Predicted Class: [18]. Word:  as\n",
            "Predicted Class: [3]. Word:  a\n",
            "Predicted Class: [211]. Word:  rose\n",
            "Predicted Class: [211]. Word:  rose\n",
            "Predicted Class: [141]. Word:  hall\n",
            "Predicted Class: [52]. Word:  me\n",
            "Predicted Class: [52]. Word:  me\n",
            "Predicted Class: [43]. Word:  relations\n",
            "Predicted Class: [43]. Word:  relations\n",
            "Predicted Class: [59]. Word:  dublin\n",
            "Predicted Class: [38]. Word:  steps\n",
            "Predicted Class: [11]. Word:  were\n",
            "Predicted Class: [7]. Word:  for\n",
            "Predicted Class: [2]. Word:  the\n",
            "Predicted Class: [124]. Word:  ladies\n",
            "Predicted Class: [124]. Word:  ladies\n",
            "Predicted Class: [103]. Word:  invitation\n",
            "Predicted Class: [136]. Word:  water\n",
            "Predicted Class: [64]. Word:  fainted\n",
            "Predicted Class: [3]. Word:  a\n",
            "Predicted Class: [116]. Word:  call\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [173]. Word:  academy\n",
            "Predicted Class: [173]. Word:  academy\n",
            "Predicted Class: [173]. Word:  academy\n",
            "Predicted Class: [141]. Word:  hall\n",
            "Predicted Class: [52]. Word:  me\n",
            "Predicted Class: [52]. Word:  me\n",
            "Predicted Class: [52]. Word:  me\n",
            "Predicted Class: [52]. Word:  me\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Predicted Class: [97]. Word:  glisten\n",
            "Laurence went to dublin the left leg from under the phelim mchugh hall steps steps ball for lanigans ball ball to the ladies ladies hall a call call eyes glisten glisten glisten glisten glisten glisten glisten glisten glisten glisten kinds hoops bellows mcgilligan the further further hall steps steps ball at as plenty as red as as red as a rose rose red as a rose rose hall me me relations relations dublin steps were for the ladies ladies invitation water fainted a call glisten glisten glisten glisten glisten glisten academy academy academy hall me me me me glisten glisten glisten glisten glisten glisten\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcHFuFe7F-eV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}